{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ada367d",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Linear regression by using Deep Neural network: Implement Boston housing price\n",
    "predictionproblem by Linear regression using Deep Neural network. Use Boston House price\n",
    "prediction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47e94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff9b424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90    NaN  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('HousingData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290337cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       20\n",
       "ZN         20\n",
       "INDUS      20\n",
       "CHAS       20\n",
       "NOX         0\n",
       "RM          0\n",
       "AGE        20\n",
       "DIS         0\n",
       "RAD         0\n",
       "TAX         0\n",
       "PTRATIO     0\n",
       "B           0\n",
       "LSTAT      20\n",
       "MEDV        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ffd0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6091/883058449.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(mean_value, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    mean_value = df[column].mean()\n",
    "    df[column].fillna(mean_value, inplace=True)\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d99862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = df.drop('MEDV', axis=1)\n",
    "# Target Variable\n",
    "y = df['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635236e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36edf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,113</span> (39.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,113\u001b[0m (39.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,113</span> (39.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,113\u001b[0m (39.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a sequential model — layers will be added one after another\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer:\n",
    "# - 128 neurons\n",
    "# - Input shape is (13,) because Boston Housing dataset has 13 features\n",
    "# - 'relu' activation adds non-linearity, helps model learn complex patterns\n",
    "model.add(Dense(128, input_shape=(13,), activation='relu', name='dense_1'))\n",
    "\n",
    "# Second hidden layer:\n",
    "# - 64 neurons\n",
    "# - Again uses 'relu' activation for non-linearity\n",
    "# - Reduces the dimensionality, allows learning more abstract representations\n",
    "model.add(Dense(64, activation='relu', name='dense_2'))\n",
    "\n",
    "# Output layer:\n",
    "# - 1 neuron since we're predicting a single continuous value (house price)\n",
    "# - 'linear' activation means no transformation (suitable for regression)\n",
    "model.add(Dense(1, activation='linear', name='dense_output'))\n",
    "\n",
    "# Compile the model:\n",
    "# - 'adam' optimizer adjusts weights efficiently\n",
    "# - 'mse' (mean squared error) is the loss function for regression\n",
    "# - 'mae' (mean absolute error) as a metric for performance monitoring\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66eb9968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5602 - mae: 1.3569 - val_loss: 16.3760 - val_mae: 2.7608\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5470 - mae: 1.3751 - val_loss: 15.9449 - val_mae: 2.7574\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5221 - mae: 1.3650 - val_loss: 14.9066 - val_mae: 2.6197\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8368 - mae: 1.3911 - val_loss: 16.9790 - val_mae: 2.7933\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2230 - mae: 1.3406 - val_loss: 14.6808 - val_mae: 2.6515\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0698 - mae: 1.2981 - val_loss: 14.4991 - val_mae: 2.6208\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0453 - mae: 1.2715 - val_loss: 15.9498 - val_mae: 2.7639\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0263 - mae: 1.2822 - val_loss: 14.8737 - val_mae: 2.6589\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3863 - mae: 1.3641 - val_loss: 15.8965 - val_mae: 2.7625\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7592 - mae: 1.2494 - val_loss: 14.7534 - val_mae: 2.6313\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4802 - mae: 1.3906 - val_loss: 15.4957 - val_mae: 2.6574\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9348 - mae: 1.2633 - val_loss: 14.9902 - val_mae: 2.6912\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7075 - mae: 1.3665 - val_loss: 14.2640 - val_mae: 2.6064\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5628 - mae: 1.3612 - val_loss: 15.5705 - val_mae: 2.7212\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9890 - mae: 1.2927 - val_loss: 14.0815 - val_mae: 2.5743\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1990 - mae: 1.3225 - val_loss: 14.9130 - val_mae: 2.6476\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0693 - mae: 1.2808 - val_loss: 14.7446 - val_mae: 2.6147\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5571 - mae: 1.3445 - val_loss: 13.5959 - val_mae: 2.5114\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2528 - mae: 1.2924 - val_loss: 14.8038 - val_mae: 2.6280\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7560 - mae: 1.1816 - val_loss: 15.3145 - val_mae: 2.6864\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8640 - mae: 1.2517 - val_loss: 14.4325 - val_mae: 2.6128\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3744 - mae: 1.3589 - val_loss: 14.6638 - val_mae: 2.6184\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3211 - mae: 1.3027 - val_loss: 14.6107 - val_mae: 2.6151\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6617 - mae: 1.1892 - val_loss: 14.3562 - val_mae: 2.6063\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8374 - mae: 1.1994 - val_loss: 14.1391 - val_mae: 2.5997\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8649 - mae: 1.2720 - val_loss: 14.3581 - val_mae: 2.5430\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4815 - mae: 1.3126 - val_loss: 15.3104 - val_mae: 2.6484\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6999 - mae: 1.2097 - val_loss: 13.9894 - val_mae: 2.5280\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1855 - mae: 1.2988 - val_loss: 14.6686 - val_mae: 2.6096\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2935 - mae: 1.1051 - val_loss: 13.8293 - val_mae: 2.5080\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2085 - mae: 1.2761 - val_loss: 14.7301 - val_mae: 2.6162\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8414 - mae: 1.2154 - val_loss: 14.0654 - val_mae: 2.5809\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6928 - mae: 1.1654 - val_loss: 13.4290 - val_mae: 2.4759\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9619 - mae: 1.2269 - val_loss: 14.2661 - val_mae: 2.6025\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1100 - mae: 1.2570 - val_loss: 14.8184 - val_mae: 2.6084\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4887 - mae: 1.1362 - val_loss: 13.2047 - val_mae: 2.4661\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5492 - mae: 1.1509 - val_loss: 14.4042 - val_mae: 2.5791\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0507 - mae: 1.2369 - val_loss: 13.5696 - val_mae: 2.5162\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3911 - mae: 1.1073 - val_loss: 13.1426 - val_mae: 2.5237\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9905 - mae: 1.2566 - val_loss: 14.0116 - val_mae: 2.4772\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7220 - mae: 1.1933 - val_loss: 14.4738 - val_mae: 2.6075\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1823 - mae: 1.2756 - val_loss: 13.1347 - val_mae: 2.4912\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0070 - mae: 1.2449 - val_loss: 14.3282 - val_mae: 2.5317\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0523 - mae: 1.1947 - val_loss: 14.3583 - val_mae: 2.5665\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9459 - mae: 1.2067 - val_loss: 12.5105 - val_mae: 2.3997\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4280 - mae: 1.0839 - val_loss: 13.8544 - val_mae: 2.5542\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3865 - mae: 1.1057 - val_loss: 12.9127 - val_mae: 2.4106\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5766 - mae: 1.1738 - val_loss: 13.6694 - val_mae: 2.5523\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4404 - mae: 1.0935 - val_loss: 13.9270 - val_mae: 2.5367\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5337 - mae: 1.1242 - val_loss: 13.1579 - val_mae: 2.4436\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6781 - mae: 1.1244 - val_loss: 13.4281 - val_mae: 2.5532\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5944 - mae: 1.1111 - val_loss: 12.9284 - val_mae: 2.4429\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6297 - mae: 1.1736 - val_loss: 13.8370 - val_mae: 2.5931\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4383 - mae: 1.1084 - val_loss: 12.3998 - val_mae: 2.3819\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4488 - mae: 1.1095 - val_loss: 13.1742 - val_mae: 2.5017\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4161 - mae: 1.0617 - val_loss: 13.2749 - val_mae: 2.5210\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0498 - mae: 1.2133 - val_loss: 13.9639 - val_mae: 2.6192\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3177 - mae: 1.0836 - val_loss: 12.6450 - val_mae: 2.4066\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5572 - mae: 1.0663 - val_loss: 12.4428 - val_mae: 2.3759\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6110 - mae: 1.1715 - val_loss: 14.5720 - val_mae: 2.6045\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0591 - mae: 1.2227 - val_loss: 12.5534 - val_mae: 2.4525\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5212 - mae: 1.1075 - val_loss: 12.7174 - val_mae: 2.4053\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7236 - mae: 1.1797 - val_loss: 13.1390 - val_mae: 2.4524\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1559 - mae: 1.0025 - val_loss: 13.9206 - val_mae: 2.5492\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0071 - mae: 1.0148 - val_loss: 12.1837 - val_mae: 2.3387\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6324 - mae: 1.1915 - val_loss: 13.8020 - val_mae: 2.6265\n",
      "Epoch 67/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4866 - mae: 1.1459 - val_loss: 14.6678 - val_mae: 2.5247\n",
      "Epoch 68/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7744 - mae: 1.2192 - val_loss: 12.2830 - val_mae: 2.4436\n",
      "Epoch 69/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7087 - mae: 1.1516 - val_loss: 12.2781 - val_mae: 2.3375\n",
      "Epoch 70/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5760 - mae: 1.1540 - val_loss: 14.4422 - val_mae: 2.7024\n",
      "Epoch 71/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4426 - mae: 1.1187 - val_loss: 11.9040 - val_mae: 2.3453\n",
      "Epoch 72/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4506 - mae: 1.1434 - val_loss: 14.1130 - val_mae: 2.5994\n",
      "Epoch 73/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0919 - mae: 1.0314 - val_loss: 13.1673 - val_mae: 2.4683\n",
      "Epoch 74/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6229 - mae: 1.1699 - val_loss: 12.5065 - val_mae: 2.4344\n",
      "Epoch 75/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2829 - mae: 1.0485 - val_loss: 12.7385 - val_mae: 2.4540\n",
      "Epoch 76/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0581 - mae: 0.9660 - val_loss: 12.8621 - val_mae: 2.3825\n",
      "Epoch 77/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1584 - mae: 1.0266 - val_loss: 12.8826 - val_mae: 2.5486\n",
      "Epoch 78/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3357 - mae: 1.0864 - val_loss: 13.5107 - val_mae: 2.5613\n",
      "Epoch 79/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0298 - mae: 1.0065 - val_loss: 12.9460 - val_mae: 2.5762\n",
      "Epoch 80/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2589 - mae: 1.0487 - val_loss: 13.0764 - val_mae: 2.4283\n",
      "Epoch 81/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1219 - mae: 1.0258 - val_loss: 13.5802 - val_mae: 2.5781\n",
      "Epoch 82/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1091 - mae: 1.0316 - val_loss: 12.3576 - val_mae: 2.4375\n",
      "Epoch 83/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9995 - mae: 0.9952 - val_loss: 12.8509 - val_mae: 2.4757\n",
      "Epoch 84/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2936 - mae: 1.0608 - val_loss: 13.7919 - val_mae: 2.6375\n",
      "Epoch 85/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5473 - mae: 1.0886 - val_loss: 12.6087 - val_mae: 2.3759\n",
      "Epoch 86/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0433 - mae: 1.0068 - val_loss: 13.1354 - val_mae: 2.5459\n",
      "Epoch 87/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0561 - mae: 1.0336 - val_loss: 13.5556 - val_mae: 2.5583\n",
      "Epoch 88/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8654 - mae: 0.9683 - val_loss: 12.9538 - val_mae: 2.4616\n",
      "Epoch 89/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2192 - mae: 1.0604 - val_loss: 12.2910 - val_mae: 2.4818\n",
      "Epoch 90/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0255 - mae: 1.0140 - val_loss: 13.3774 - val_mae: 2.4772\n",
      "Epoch 91/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3070 - mae: 1.0885 - val_loss: 13.6119 - val_mae: 2.5788\n",
      "Epoch 92/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0838 - mae: 0.9865 - val_loss: 13.1546 - val_mae: 2.5571\n",
      "Epoch 93/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2525 - mae: 1.0597 - val_loss: 12.6089 - val_mae: 2.4319\n",
      "Epoch 94/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0927 - mae: 1.0061 - val_loss: 12.9248 - val_mae: 2.4844\n",
      "Epoch 95/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1502 - mae: 0.9783 - val_loss: 12.4920 - val_mae: 2.4180\n",
      "Epoch 96/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5305 - mae: 1.0653 - val_loss: 14.4142 - val_mae: 2.7469\n",
      "Epoch 97/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3457 - mae: 1.0931 - val_loss: 13.6348 - val_mae: 2.5298\n",
      "Epoch 98/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9948 - mae: 1.0496 - val_loss: 12.0899 - val_mae: 2.4663\n",
      "Epoch 99/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0941 - mae: 1.0543 - val_loss: 12.3338 - val_mae: 2.4656\n",
      "Epoch 100/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8856 - mae: 0.9731 - val_loss: 13.7844 - val_mae: 2.5566\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled,     # Scaled input features (13 features per sample)\n",
    "    y_train,            # Target house prices for training\n",
    "    epochs=100,         # Number of times the model sees the full dataset (training iterations)\n",
    "    validation_split=0.05,  # Use 5% of the training data for validation to monitor overfitting\n",
    "    verbose=1           # Display progress during training (1 = progress bar)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc5cad12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3.5257174968719482,\n",
       "  3.5400660037994385,\n",
       "  3.454930543899536,\n",
       "  3.4836654663085938,\n",
       "  3.4606456756591797,\n",
       "  3.446512222290039,\n",
       "  3.333364248275757,\n",
       "  3.358428478240967,\n",
       "  3.3813672065734863,\n",
       "  3.2268407344818115,\n",
       "  3.3519585132598877,\n",
       "  3.190944194793701,\n",
       "  3.2718234062194824,\n",
       "  3.3398165702819824,\n",
       "  3.3135173320770264,\n",
       "  3.2601659297943115,\n",
       "  3.1437525749206543,\n",
       "  3.2018308639526367,\n",
       "  3.1302828788757324,\n",
       "  3.1498801708221436,\n",
       "  3.1604814529418945,\n",
       "  3.080937385559082,\n",
       "  3.0500245094299316,\n",
       "  3.0010921955108643,\n",
       "  3.1394786834716797,\n",
       "  3.1273226737976074,\n",
       "  3.0660769939422607,\n",
       "  2.9916770458221436,\n",
       "  2.852759838104248,\n",
       "  2.856718063354492,\n",
       "  2.909921646118164,\n",
       "  2.946471929550171,\n",
       "  2.8476974964141846,\n",
       "  2.8267290592193604,\n",
       "  2.8432228565216064,\n",
       "  2.8084909915924072,\n",
       "  2.868276596069336,\n",
       "  2.92755126953125,\n",
       "  2.7861480712890625,\n",
       "  2.867090940475464,\n",
       "  2.867445707321167,\n",
       "  2.974402904510498,\n",
       "  2.7693088054656982,\n",
       "  2.802849292755127,\n",
       "  2.722914695739746,\n",
       "  2.6920387744903564,\n",
       "  2.65185284614563,\n",
       "  2.6740832328796387,\n",
       "  2.6127500534057617,\n",
       "  2.623241901397705,\n",
       "  2.5031635761260986,\n",
       "  2.616328001022339,\n",
       "  2.625328302383423,\n",
       "  2.5042800903320312,\n",
       "  2.6090004444122314,\n",
       "  2.5473105907440186,\n",
       "  2.6214892864227295,\n",
       "  2.440690517425537,\n",
       "  2.4885001182556152,\n",
       "  2.627896785736084,\n",
       "  2.6080517768859863,\n",
       "  2.564851999282837,\n",
       "  2.4409987926483154,\n",
       "  2.376340627670288,\n",
       "  2.405036449432373,\n",
       "  2.6487157344818115,\n",
       "  2.5904858112335205,\n",
       "  2.688446521759033,\n",
       "  2.4735167026519775,\n",
       "  2.6919105052948,\n",
       "  2.4274535179138184,\n",
       "  2.336470127105713,\n",
       "  2.3589961528778076,\n",
       "  2.342495918273926,\n",
       "  2.344839096069336,\n",
       "  2.3518130779266357,\n",
       "  2.3126606941223145,\n",
       "  2.294973373413086,\n",
       "  2.287715196609497,\n",
       "  2.2712130546569824,\n",
       "  2.203888416290283,\n",
       "  2.1990342140197754,\n",
       "  2.1689088344573975,\n",
       "  2.2334179878234863,\n",
       "  2.1547839641571045,\n",
       "  2.157146692276001,\n",
       "  2.1658825874328613,\n",
       "  2.151169538497925,\n",
       "  2.2339320182800293,\n",
       "  2.272282123565674,\n",
       "  2.2075047492980957,\n",
       "  2.143479585647583,\n",
       "  2.1304030418395996,\n",
       "  2.1021831035614014,\n",
       "  2.0444891452789307,\n",
       "  2.1464004516601562,\n",
       "  2.384667158126831,\n",
       "  2.1017894744873047,\n",
       "  2.082399368286133,\n",
       "  2.0716845989227295],\n",
       " 'mae': [1.3696823120117188,\n",
       "  1.3725595474243164,\n",
       "  1.348651647567749,\n",
       "  1.3886545896530151,\n",
       "  1.3635345697402954,\n",
       "  1.347740888595581,\n",
       "  1.344815969467163,\n",
       "  1.328568696975708,\n",
       "  1.3674992322921753,\n",
       "  1.3033944368362427,\n",
       "  1.3577808141708374,\n",
       "  1.2886087894439697,\n",
       "  1.3055425882339478,\n",
       "  1.3395740985870361,\n",
       "  1.3288567066192627,\n",
       "  1.3109898567199707,\n",
       "  1.3028677701950073,\n",
       "  1.299231767654419,\n",
       "  1.2844072580337524,\n",
       "  1.2688372135162354,\n",
       "  1.2866771221160889,\n",
       "  1.2743241786956787,\n",
       "  1.2510751485824585,\n",
       "  1.2540889978408813,\n",
       "  1.2739273309707642,\n",
       "  1.2877699136734009,\n",
       "  1.2779451608657837,\n",
       "  1.2570898532867432,\n",
       "  1.2044695615768433,\n",
       "  1.2048234939575195,\n",
       "  1.2312718629837036,\n",
       "  1.2380971908569336,\n",
       "  1.208725929260254,\n",
       "  1.1839525699615479,\n",
       "  1.204620122909546,\n",
       "  1.200613260269165,\n",
       "  1.211620807647705,\n",
       "  1.217720627784729,\n",
       "  1.1890075206756592,\n",
       "  1.2341222763061523,\n",
       "  1.2414144277572632,\n",
       "  1.2728897333145142,\n",
       "  1.220031499862671,\n",
       "  1.1961170434951782,\n",
       "  1.1774016618728638,\n",
       "  1.15468430519104,\n",
       "  1.1380929946899414,\n",
       "  1.1563996076583862,\n",
       "  1.1450753211975098,\n",
       "  1.1394938230514526,\n",
       "  1.1166479587554932,\n",
       "  1.1356028318405151,\n",
       "  1.140485167503357,\n",
       "  1.11814546585083,\n",
       "  1.1518609523773193,\n",
       "  1.1219488382339478,\n",
       "  1.150109887123108,\n",
       "  1.0972251892089844,\n",
       "  1.0863629579544067,\n",
       "  1.1632399559020996,\n",
       "  1.1481918096542358,\n",
       "  1.1482069492340088,\n",
       "  1.1075905561447144,\n",
       "  1.0828043222427368,\n",
       "  1.1096922159194946,\n",
       "  1.175490379333496,\n",
       "  1.1591023206710815,\n",
       "  1.179690957069397,\n",
       "  1.1274157762527466,\n",
       "  1.1877965927124023,\n",
       "  1.1276496648788452,\n",
       "  1.0961679220199585,\n",
       "  1.081019401550293,\n",
       "  1.0875123739242554,\n",
       "  1.0551164150238037,\n",
       "  1.085200309753418,\n",
       "  1.0657997131347656,\n",
       "  1.0526329278945923,\n",
       "  1.0400842428207397,\n",
       "  1.0684019327163696,\n",
       "  1.0482958555221558,\n",
       "  1.0364089012145996,\n",
       "  1.0365177392959595,\n",
       "  1.044222116470337,\n",
       "  1.0115787982940674,\n",
       "  1.0138789415359497,\n",
       "  1.0333150625228882,\n",
       "  1.025351881980896,\n",
       "  1.046931505203247,\n",
       "  1.0770268440246582,\n",
       "  1.0508067607879639,\n",
       "  1.0095516443252563,\n",
       "  1.0260388851165771,\n",
       "  1.012431025505066,\n",
       "  0.9786776304244995,\n",
       "  1.0156317949295044,\n",
       "  1.0947697162628174,\n",
       "  1.0231797695159912,\n",
       "  1.018753170967102,\n",
       "  1.0181338787078857],\n",
       " 'val_loss': [16.375959396362305,\n",
       "  15.944859504699707,\n",
       "  14.906574249267578,\n",
       "  16.97902488708496,\n",
       "  14.68080997467041,\n",
       "  14.499110221862793,\n",
       "  15.949756622314453,\n",
       "  14.873726844787598,\n",
       "  15.896512985229492,\n",
       "  14.75339412689209,\n",
       "  15.495734214782715,\n",
       "  14.990230560302734,\n",
       "  14.263996124267578,\n",
       "  15.570521354675293,\n",
       "  14.081527709960938,\n",
       "  14.91297435760498,\n",
       "  14.744576454162598,\n",
       "  13.595937728881836,\n",
       "  14.803763389587402,\n",
       "  15.314536094665527,\n",
       "  14.43253231048584,\n",
       "  14.663796424865723,\n",
       "  14.610736846923828,\n",
       "  14.356184005737305,\n",
       "  14.139093399047852,\n",
       "  14.35808277130127,\n",
       "  15.31035041809082,\n",
       "  13.989364624023438,\n",
       "  14.668563842773438,\n",
       "  13.829294204711914,\n",
       "  14.730137825012207,\n",
       "  14.065398216247559,\n",
       "  13.429020881652832,\n",
       "  14.266096115112305,\n",
       "  14.818381309509277,\n",
       "  13.204680442810059,\n",
       "  14.404159545898438,\n",
       "  13.569554328918457,\n",
       "  13.142642974853516,\n",
       "  14.011571884155273,\n",
       "  14.47384262084961,\n",
       "  13.134665489196777,\n",
       "  14.32816219329834,\n",
       "  14.358257293701172,\n",
       "  12.51047420501709,\n",
       "  13.85436725616455,\n",
       "  12.912686347961426,\n",
       "  13.6693696975708,\n",
       "  13.926972389221191,\n",
       "  13.157906532287598,\n",
       "  13.428082466125488,\n",
       "  12.928433418273926,\n",
       "  13.837047576904297,\n",
       "  12.399799346923828,\n",
       "  13.17420482635498,\n",
       "  13.27493667602539,\n",
       "  13.963910102844238,\n",
       "  12.645024299621582,\n",
       "  12.442760467529297,\n",
       "  14.571962356567383,\n",
       "  12.553423881530762,\n",
       "  12.717392921447754,\n",
       "  13.13896656036377,\n",
       "  13.920625686645508,\n",
       "  12.183671951293945,\n",
       "  13.802027702331543,\n",
       "  14.667814254760742,\n",
       "  12.282952308654785,\n",
       "  12.27810001373291,\n",
       "  14.442179679870605,\n",
       "  11.90400218963623,\n",
       "  14.113021850585938,\n",
       "  13.167312622070312,\n",
       "  12.506475448608398,\n",
       "  12.738530158996582,\n",
       "  12.8621244430542,\n",
       "  12.882559776306152,\n",
       "  13.510676383972168,\n",
       "  12.946024894714355,\n",
       "  13.076436042785645,\n",
       "  13.580153465270996,\n",
       "  12.357630729675293,\n",
       "  12.850915908813477,\n",
       "  13.79186725616455,\n",
       "  12.608672142028809,\n",
       "  13.13538646697998,\n",
       "  13.555608749389648,\n",
       "  12.953848838806152,\n",
       "  12.291024208068848,\n",
       "  13.377443313598633,\n",
       "  13.611885070800781,\n",
       "  13.154557228088379,\n",
       "  12.608851432800293,\n",
       "  12.924782752990723,\n",
       "  12.491984367370605,\n",
       "  14.414162635803223,\n",
       "  13.634822845458984,\n",
       "  12.089874267578125,\n",
       "  12.33378791809082,\n",
       "  13.78443431854248],\n",
       " 'val_mae': [2.760787010192871,\n",
       "  2.757412910461426,\n",
       "  2.6197314262390137,\n",
       "  2.7932910919189453,\n",
       "  2.6514642238616943,\n",
       "  2.6207571029663086,\n",
       "  2.763939380645752,\n",
       "  2.65889835357666,\n",
       "  2.7625017166137695,\n",
       "  2.631258010864258,\n",
       "  2.6573987007141113,\n",
       "  2.6911725997924805,\n",
       "  2.606426239013672,\n",
       "  2.721231460571289,\n",
       "  2.5742743015289307,\n",
       "  2.6476473808288574,\n",
       "  2.614706039428711,\n",
       "  2.5113890171051025,\n",
       "  2.628035545349121,\n",
       "  2.686392068862915,\n",
       "  2.6128270626068115,\n",
       "  2.6184117794036865,\n",
       "  2.6151275634765625,\n",
       "  2.606339454650879,\n",
       "  2.599717378616333,\n",
       "  2.542971611022949,\n",
       "  2.648449420928955,\n",
       "  2.528032064437866,\n",
       "  2.609567880630493,\n",
       "  2.507970094680786,\n",
       "  2.616180658340454,\n",
       "  2.58087158203125,\n",
       "  2.4759225845336914,\n",
       "  2.602508306503296,\n",
       "  2.6083567142486572,\n",
       "  2.4660634994506836,\n",
       "  2.5790834426879883,\n",
       "  2.5161890983581543,\n",
       "  2.5237107276916504,\n",
       "  2.4772443771362305,\n",
       "  2.6075150966644287,\n",
       "  2.4911680221557617,\n",
       "  2.5316736698150635,\n",
       "  2.566549062728882,\n",
       "  2.3997209072113037,\n",
       "  2.5542123317718506,\n",
       "  2.41062331199646,\n",
       "  2.5522706508636475,\n",
       "  2.5367331504821777,\n",
       "  2.4435622692108154,\n",
       "  2.5532100200653076,\n",
       "  2.442934989929199,\n",
       "  2.593135356903076,\n",
       "  2.38185715675354,\n",
       "  2.501746416091919,\n",
       "  2.5209591388702393,\n",
       "  2.6192049980163574,\n",
       "  2.406585216522217,\n",
       "  2.3759171962738037,\n",
       "  2.6044771671295166,\n",
       "  2.452517509460449,\n",
       "  2.40527081489563,\n",
       "  2.452364921569824,\n",
       "  2.549212694168091,\n",
       "  2.338714838027954,\n",
       "  2.6264703273773193,\n",
       "  2.5247066020965576,\n",
       "  2.443633556365967,\n",
       "  2.337521553039551,\n",
       "  2.7023844718933105,\n",
       "  2.3452565670013428,\n",
       "  2.599432945251465,\n",
       "  2.4682607650756836,\n",
       "  2.434375524520874,\n",
       "  2.4539637565612793,\n",
       "  2.382535696029663,\n",
       "  2.548572063446045,\n",
       "  2.5612759590148926,\n",
       "  2.5762250423431396,\n",
       "  2.4282917976379395,\n",
       "  2.578108549118042,\n",
       "  2.4374618530273438,\n",
       "  2.475739002227783,\n",
       "  2.637547492980957,\n",
       "  2.3758983612060547,\n",
       "  2.545917510986328,\n",
       "  2.558277130126953,\n",
       "  2.461617946624756,\n",
       "  2.4818389415740967,\n",
       "  2.4771881103515625,\n",
       "  2.578786849975586,\n",
       "  2.557053565979004,\n",
       "  2.4319162368774414,\n",
       "  2.4844279289245605,\n",
       "  2.417997121810913,\n",
       "  2.746947765350342,\n",
       "  2.5298471450805664,\n",
       "  2.466292381286621,\n",
       "  2.465571165084839,\n",
       "  2.5566110610961914]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a03e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.9150 - mae: 2.5906\n",
      "Mean squared error on test data:  13.9882230758667\n",
      "Mean absolute error on test data:  2.7374157905578613\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the scaled test data\n",
    "# Returns loss (MSE) and metrics (MAE, as specified during model compilation)\n",
    "mse_nn, mae_nn = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Mean squared error on test data: ', mse_nn)  # Measures average of squared prediction errors\n",
    "print('Mean absolute error on test data: ', mae_nn)  # Measures average of absolute prediction errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
